{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mrQBO-fJiMBk"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import fastBPE\n",
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "tX4DYrl0jn44"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "yyQrrB1SLzvg"
   },
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "# device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "DTqmWQO-RtXD",
    "outputId": "94ad1ff4-7337-4ffb-db2a-041e185d0b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: indic-nlp-library in /home/maxslide/.local/lib/python3.8/site-packages (0.71)\n",
      "Requirement already satisfied: pandas in /home/maxslide/.local/lib/python3.8/site-packages (from indic-nlp-library) (1.1.5)\n",
      "Requirement already satisfied: morfessor in /home/maxslide/.local/lib/python3.8/site-packages (from indic-nlp-library) (2.0.6)\n",
      "Requirement already satisfied: numpy in /home/maxslide/.local/lib/python3.8/site-packages (from indic-nlp-library) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas->indic-nlp-library) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas->indic-nlp-library) (2019.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "SbV9ga2cQffV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Batman vs Superman</td>\n",
       "      <td>batman vs superman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The director is Zack Snyder, 27% Rotten Tomato...</td>\n",
       "      <td>Zack Snyder director hai, 27% Rotten Tomatoes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not very popular it seems</td>\n",
       "      <td>lagta hai bahut popular nahi hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But the audiences liked it. It has a B cinema ...</td>\n",
       "      <td>but audience ne like kiya, iska cinema score B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>Ok.</td>\n",
       "      <td>Thik hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8056</th>\n",
       "      <td>ok bro</td>\n",
       "      <td>Thik hai bhai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8057</th>\n",
       "      <td>shall we continue?</td>\n",
       "      <td>Kya ham chalu kar sakte hai?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>do you like we can</td>\n",
       "      <td>Kya aapko pasand hai hamare saath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>Yeah I have fifteen thanks.</td>\n",
       "      <td>Haa mere pass pandrah thanks hai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8060 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                english  \\\n",
       "0                                    Batman vs Superman   \n",
       "1     The director is Zack Snyder, 27% Rotten Tomato...   \n",
       "2                             Not very popular it seems   \n",
       "3     But the audiences liked it. It has a B cinema ...   \n",
       "4                                                   Yes   \n",
       "...                                                 ...   \n",
       "8055                                                Ok.   \n",
       "8056                                             ok bro   \n",
       "8057                                 shall we continue?   \n",
       "8058                                 do you like we can   \n",
       "8059                        Yeah I have fifteen thanks.   \n",
       "\n",
       "                                                  hindi  \n",
       "0                                    batman vs superman  \n",
       "1     Zack Snyder director hai, 27% Rotten Tomatoes,...  \n",
       "2                      lagta hai bahut popular nahi hai  \n",
       "3     but audience ne like kiya, iska cinema score B...  \n",
       "4                                                   yes  \n",
       "...                                                 ...  \n",
       "8055                                           Thik hai  \n",
       "8056                                      Thik hai bhai  \n",
       "8057                       Kya ham chalu kar sakte hai?  \n",
       "8058                  Kya aapko pasand hai hamare saath  \n",
       "8059                   Haa mere pass pandrah thanks hai  \n",
       "\n",
       "[8060 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from indicnlp.tokenize import indic_tokenize\n",
    "# df = pd.read_pickle(\"./en_hi.pkl\")\n",
    "# en_tokenizer = spacy.load('en')\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "# df = pd.read_pickle(\"./en_hi.pkl\")\n",
    "df = pd.read_csv('./train.txt', delimiter = \"\\t\", header = None)\n",
    "df.rename(columns = {0 : 'english', 1 : 'hindi'}, inplace = True)\n",
    "en_tokenizer = spacy.load('en_core_web_sm')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(corpus: str) -> dict:\n",
    "    \"\"\"Step 1. Build vocab from text corpus\"\"\"\n",
    "\n",
    "    # Separate each char in word by space and add mark end of token\n",
    "    tokens = [\" \".join(word) + \" </w>\" for word in corpus.split()]\n",
    "    \n",
    "    # Count frequency of tokens in corpus\n",
    "    vocab = Counter(tokens)  \n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def get_stats(vocab: dict) -> dict:\n",
    "    \"\"\"Step 2. Get counts of pairs of consecutive symbols\"\"\"\n",
    "\n",
    "    pairs = defaultdict(int)\n",
    "    for word, frequency in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        # Counting up occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += frequency\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair: tuple, v_in: dict) -> dict:\n",
    "    \"\"\"Step 3. Merge all occurrences of the most frequent pair\"\"\"\n",
    "    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    final_p.append([p,pair])\n",
    "    for word in v_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "\n",
    "    return v_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_p = []\n",
    "vocab = build_vocab(\" \".join(df['english']))  # Step 1\n",
    "\n",
    "num_merges = 300  # Hyperparameter\n",
    "for i in range(num_merges):\n",
    "\n",
    "    pairs = get_stats(vocab)  # Step 2\n",
    "\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # step 3\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "for i in range(len(df['english'])):\n",
    "    sentence = df['english'][i]\n",
    "    tokens = [\" \".join(word) + \" </w>\" for word in sentence.split()]\n",
    "    for j in range(len(tokens)):\n",
    "        for k in final_p:\n",
    "            temp = k[0].sub(''.join(k[1]), tokens[j])\n",
    "            tokens[j] = temp\n",
    "    df['english'][i] = \" \".join(tokens)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_p = []\n",
    "vocab = build_vocab(\" \".join(df['hindi']))  # Step 1\n",
    "\n",
    "num_merges = 300  # Hyperparameter\n",
    "for i in range(num_merges):\n",
    "\n",
    "    pairs = get_stats(vocab)  # Step 2\n",
    "\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # step 3\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "for i in range(len(df['hindi'])):\n",
    "    sentence = df['hindi'][i]\n",
    "    tokens = [\" \".join(word) + \" </w>\" for word in sentence.split()]\n",
    "    for j in range(len(tokens)):\n",
    "        for k in final_p:\n",
    "            temp = k[0].sub(''.join(k[1]), tokens[j])\n",
    "            tokens[j] = temp\n",
    "    df['hindi'][i] = \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B at man&lt;/w&gt; v s&lt;/w&gt; S u per man&lt;/w&gt;</td>\n",
       "      <td>ba t man&lt;/w&gt; v s&lt;/w&gt; su per man&lt;/w&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The&lt;/w&gt; di rec t or&lt;/w&gt; is&lt;/w&gt; Z ac k&lt;/w&gt; S n ...</td>\n",
       "      <td>Z ac k&lt;/w&gt; S n y de r&lt;/w&gt; di re c to r&lt;/w&gt; hai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N ot&lt;/w&gt; very&lt;/w&gt; p op ul ar&lt;/w&gt; it&lt;/w&gt; se em ...</td>\n",
       "      <td>lagta&lt;/w&gt; hai&lt;/w&gt; bahut&lt;/w&gt; po p u l ar&lt;/w&gt; na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B u t&lt;/w&gt; the&lt;/w&gt; a u di en c es&lt;/w&gt; li ked&lt;/w...</td>\n",
       "      <td>b ut&lt;/w&gt; au di en ce&lt;/w&gt; ne&lt;/w&gt; li ke&lt;/w&gt; ki y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y es&lt;/w&gt;</td>\n",
       "      <td>ye s&lt;/w&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8055</th>\n",
       "      <td>O k .&lt;/w&gt;</td>\n",
       "      <td>T hi k&lt;/w&gt; hai&lt;/w&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8056</th>\n",
       "      <td>o k&lt;/w&gt; b r o&lt;/w&gt;</td>\n",
       "      <td>T hi k&lt;/w&gt; hai&lt;/w&gt; b hai&lt;/w&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8057</th>\n",
       "      <td>s ha ll&lt;/w&gt; w e&lt;/w&gt; con t in u e?&lt;/w&gt;</td>\n",
       "      <td>Kya&lt;/w&gt; ha m&lt;/w&gt; cha l u &lt;/w&gt; kar&lt;/w&gt; sak te&lt;/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>do&lt;/w&gt; you&lt;/w&gt; like&lt;/w&gt; w e&lt;/w&gt; c an&lt;/w&gt;</td>\n",
       "      <td>Kya&lt;/w&gt; aap ko&lt;/w&gt; pasand&lt;/w&gt; hai&lt;/w&gt; ha m are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>Yea h&lt;/w&gt; I&lt;/w&gt; have&lt;/w&gt; f i f te en&lt;/w&gt; th an...</td>\n",
       "      <td>H a a&lt;/w&gt; me re&lt;/w&gt; pas s&lt;/w&gt; p an d r ah&lt;/w&gt; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8060 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                english  \\\n",
       "0                  B at man</w> v s</w> S u per man</w>   \n",
       "1     The</w> di rec t or</w> is</w> Z ac k</w> S n ...   \n",
       "2     N ot</w> very</w> p op ul ar</w> it</w> se em ...   \n",
       "3     B u t</w> the</w> a u di en c es</w> li ked</w...   \n",
       "4                                              Y es</w>   \n",
       "...                                                 ...   \n",
       "8055                                          O k .</w>   \n",
       "8056                                  o k</w> b r o</w>   \n",
       "8057              s ha ll</w> w e</w> con t in u e?</w>   \n",
       "8058           do</w> you</w> like</w> w e</w> c an</w>   \n",
       "8059  Yea h</w> I</w> have</w> f i f te en</w> th an...   \n",
       "\n",
       "                                                  hindi  \n",
       "0                   ba t man</w> v s</w> su per man</w>  \n",
       "1     Z ac k</w> S n y de r</w> di re c to r</w> hai...  \n",
       "2     lagta</w> hai</w> bahut</w> po p u l ar</w> na...  \n",
       "3     b ut</w> au di en ce</w> ne</w> li ke</w> ki y...  \n",
       "4                                              ye s</w>  \n",
       "...                                                 ...  \n",
       "8055                                 T hi k</w> hai</w>  \n",
       "8056                       T hi k</w> hai</w> b hai</w>  \n",
       "8057  Kya</w> ha m</w> cha l u </w> kar</w> sak te</...  \n",
       "8058  Kya</w> aap ko</w> pasand</w> hai</w> ha m are...  \n",
       "8059  H a a</w> me re</w> pas s</w> p an d r ah</w> ...  \n",
       "\n",
       "[8060 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "FKnjcI_mZMJM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello there, I have not seen this movie so im ...</td>\n",
       "      <td>hello yar, mein is movie ko nahi dekha hoon th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alright that is fine. What is the movie?</td>\n",
       "      <td>acha tho is movie kis baare me hein?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The movie is The Social Network</td>\n",
       "      <td>is movie tho social network ke bare mein hein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have not seen that one either.</td>\n",
       "      <td>mein aise kuch nahi dekha hoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>Yep. Thanks for chatting</td>\n",
       "      <td>Yep. Thanks baat karne ke liye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>thanks, I will watch it. SOunds good</td>\n",
       "      <td>Thanks, mei dekhati hui. Achi baat hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>Hi!</td>\n",
       "      <td>Hi!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>Did you like the movie Despicable Me?</td>\n",
       "      <td>kya tumhe movie Despicable Me pasand hai?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>I did not realize this movie came out in 2010,...</td>\n",
       "      <td>mujhe nahi lagta ki ye movie 2010 me aayi thi,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>942 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               english  \\\n",
       "0                                                hello   \n",
       "1    hello there, I have not seen this movie so im ...   \n",
       "2             Alright that is fine. What is the movie?   \n",
       "3                      The movie is The Social Network   \n",
       "4                     I have not seen that one either.   \n",
       "..                                                 ...   \n",
       "937                           Yep. Thanks for chatting   \n",
       "938               thanks, I will watch it. SOunds good   \n",
       "939                                                Hi!   \n",
       "940              Did you like the movie Despicable Me?   \n",
       "941  I did not realize this movie came out in 2010,...   \n",
       "\n",
       "                                                 hindi  \n",
       "0                                                hello  \n",
       "1    hello yar, mein is movie ko nahi dekha hoon th...  \n",
       "2                 acha tho is movie kis baare me hein?  \n",
       "3        is movie tho social network ke bare mein hein  \n",
       "4                       mein aise kuch nahi dekha hoon  \n",
       "..                                                 ...  \n",
       "937                     Yep. Thanks baat karne ke liye  \n",
       "938             Thanks, mei dekhati hui. Achi baat hai  \n",
       "939                                                Hi!  \n",
       "940          kya tumhe movie Despicable Me pasand hai?  \n",
       "941  mujhe nahi lagta ki ye movie 2010 me aayi thi,...  \n",
       "\n",
       "[942 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_temp, test_data_sent = train_test_split(df, test_size = 0.2)\n",
    "# train_data_sent, validation_data_sent = train_test_split(train_temp, test_size=0.125)\n",
    "\n",
    "train_data_sent, test_data_sent = train_test_split(df, test_size = 0.2)\n",
    "validation_data_sent = pd.read_csv('./dev.txt', delimiter = \"\\t\", header = None)\n",
    "validation_data_sent.rename(columns = {0 : 'english', 1 : 'hindi'}, inplace = True)\n",
    "validation_data_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4303    ya d i</w> aap</w> a i ti ha a si k</w> ph il ...\n",
       "3807                                              o k</w>\n",
       "1088    C D A</w> tho</w> is</w> movie</w> ke</w> liye...\n",
       "2807    L e on ar d o</w> is ke</w> liye</w> O sc ar</...\n",
       "7271    a bhi</w> mujhe</w> ach ch i</w> tarah</w> se<...\n",
       "                              ...                        \n",
       "2321    haan .</w> yeh</w> tho</w> a bhi</w> bhi</w> c...\n",
       "4651    ja is i</w> ki</w> main</w> p lo t</w> k is ke...\n",
       "3583    Mujhe</w> ac tion</w> f il m s</w> pasand</w> ...\n",
       "908        Kya</w> ye</w> la m b i</w> movie</w> hai?</w>\n",
       "769     N o pe ,</w> par</w> me</w> cha h ta</w> h un ...\n",
       "Name: hindi, Length: 6448, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sent['hindi'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "TI-yItsdZgHD"
   },
   "outputs": [],
   "source": [
    "train_data_sent.to_json('train_data.json', orient='records', lines=True)\n",
    "validation_data_sent.to_json('validation_data.json', orient='records', lines=True)\n",
    "test_data_sent.to_json('test_data.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "tOZHUfjGJkCj"
   },
   "outputs": [],
   "source": [
    "def hindi_tokenizer(sentence):\n",
    "#     print(\"This is Sentence - \\n\",sentence)\n",
    "    a = [word.text for word in en_tokenizer.tokenizer(sentence.strip().split(\"-\")[-1].strip())]\n",
    "    \n",
    "#     print(a)\n",
    "    return a\n",
    "        \n",
    "#     return [word for word in indic_tokenize.trivial_tokenize(sentence.strip().split(\"-\")[-1].strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "4iizw29rJlSD"
   },
   "outputs": [],
   "source": [
    "def english_tokenizer(sentence):\n",
    "#     a = [word.text for word in en_tokenizer.tokenizer(sentence.strip().split(\"-\")[-1].strip())][::-1]\n",
    "    a = [word.text for word in en_tokenizer.tokenizer(sentence.strip().split(\"-\")[-1].strip())]\n",
    "#     print(\"This is english sentence \\n\", sentence)\n",
    "#     print(a)\n",
    "    return a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_tokenizer_subword(sentence):\n",
    "    \n",
    "    a = sentence.split()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindi_tokenizer_subword(sentence):\n",
    "#     print(\"This is Sentence - \\n\",sentence)\n",
    "    a = sentence.split()\n",
    "    return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "pLZ9U8-Ua1BL"
   },
   "outputs": [],
   "source": [
    "# HINDI = Field(tokenize = hindi_tokenizer, init_token = '<sos>', eos_token = '<eos>')\n",
    "# ENGLISH = Field(tokenize = english_tokenizer, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
    "# fields = {'english': ('english', ENGLISH), 'hindi': ('hindi', HINDI)}\n",
    "HINDI = Field(tokenize = hindi_tokenizer_subword , init_token = '<sos>', eos_token = '<eos>' , lower = True)\n",
    "ENGLISH = Field(tokenize = english_tokenizer_subword, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
    "fields = {'english': ('english', ENGLISH), 'hindi': ('hindi', HINDI)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "CjDN5eG1bNhx"
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = data.TabularDataset.splits(path = '',\n",
    "                                        train = 'train_data.json',\n",
    "                                        validation = 'validation_data.json',\n",
    "                                        test = 'test_data.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)\n",
    "\n",
    "# print(len(train_data), len(validation_data), len(test_data))\n",
    "train_data_iterator, validation_data_iterator, test_data_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size = 64, device = device, sort = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TfkuEeCSdNYJ",
    "outputId": "b24a735f-949a-4afa-b8cd-b0ba4e638043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 355, Hindi: 344\n"
     ]
    }
   ],
   "source": [
    "ENGLISH.build_vocab(train_data, min_freq=2)\n",
    "HINDI.build_vocab(train_data, min_freq=2)\n",
    "print(f\"English: {len(ENGLISH.vocab)}, Hindi: {len(HINDI.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.legacy.data.example.Example at 0x7f49eb77c970>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.examples[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "sRVU1uYxkX-9",
    "outputId": "b4b4ae20-4138-48f5-a4e0-9ee3a308e10b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': ['so</w>', 'w', 'ha', \"t's</w>\", 'ne', 'x', 't</w>'],\n",
       " 'hindi': ['a', 'b</w>', 'aur</w>', 'kya</w>', 'hai</w>']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(test_data.examples[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "VgDCXq8mhNPm"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iterator, criterion):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iterator:\n",
    "            actual = batch.hindi\n",
    "            input = batch.english\n",
    "            predictions = model(input, actual)\n",
    "            vocab_size = predictions.shape[-1]\n",
    "            predictions = predictions[1:].view(-1, vocab_size)\n",
    "            actual = actual[1:].view(-1) # flattening the sentence_length x batch_size dimensions because cross entropy loss needs 1d shape and also removing the sos token\n",
    "            total_loss += criterion(predictions, actual).item()\n",
    "    return total_loss / len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "mz5A1BlBg3oA"
   },
   "outputs": [],
   "source": [
    "def train(model, train_data_iterator, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    model.train()    \n",
    "    for batch in train_data_iterator:\n",
    "        actual = batch.hindi\n",
    "        input = batch.english\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input, actual)\n",
    "        vocab_size = predictions.shape[-1]\n",
    "        predictions = predictions[1:].view(-1, vocab_size)\n",
    "        actual = actual[1:].view(-1) # flattening the sentence_length x batch_size dimensions because cross entropy loss needs 1d shape and also removing the sos token\n",
    "        loss = criterion(predictions, actual)\n",
    "        loss.backward()   \n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "yZ_pMu-eeEjY"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, p,n):\n",
    "        super().__init__()\n",
    "        self.n=n\n",
    "        self.word_embeddings = nn.Embedding(input_size, embedding_size) \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # embeddings = self.dropout(self.word_embeddings(input)) # shape will be sentence_length x batch_size x embedding_size\n",
    "        o, (h,c) = self.lstm(self.dropout(self.word_embeddings(input))) # output shape will be sentence_length x batch_size x hidden size, hidden shape will be 1 x batch_size x hidden_size because we are returning only the final hidden layer\n",
    "        return h,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "HH8XVZ-Ygvvh"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, p,n):\n",
    "        super().__init__()\n",
    "        self.n=n\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.word_embeddings = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size + hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(embedding_size + hidden_size + hidden_size, output_size)\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input, h,c, z):        \n",
    "        embeddings = self.dropout(self.word_embeddings(input)) # shape will be 1 x batch_size x embedding_size\n",
    "        o, (h,c) = self.lstm(torch.cat((embeddings, z), dim = 2), (h,c)) # shape will be 1 x batch_size x hidden_size\n",
    "        predictions = self.linear(torch.cat((embeddings, h, z), dim = 2).squeeze(0)) # both shapes will be batch_size x output_vocabulary_size\n",
    "        return predictions, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "e3WGNx-Qg1YL"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "         \n",
    "    def forward(self, input, actual):\n",
    "        h, c = self.encoder(input)\n",
    "        z = h\n",
    "        input = actual[0, :]\n",
    "        predictions = torch.zeros(actual.shape[0], actual.shape[1], self.decoder.output_size).to(self.device)\n",
    "        for t in range(1, actual.shape[0]):\n",
    "            o, h, c = self.decoder(input.unsqueeze(0), h, c, z)\n",
    "            predictions[t] = o\n",
    "            predicted = o.argmax(1) \n",
    "            input = predicted\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "oGKiWLPchVsb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (encoder): Encoder(\n",
       "    (word_embeddings): Embedding(355, 350)\n",
       "    (lstm): LSTM(350, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (word_embeddings): Embedding(344, 350)\n",
       "    (lstm): LSTM(862, 512)\n",
       "    (linear): Linear(in_features=1374, out_features=344, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_DIM = 350\n",
    "DIM = 512\n",
    "DROPOUT = 0.5\n",
    "layer=2\n",
    "enc = Encoder(len(ENGLISH.vocab), EMB_DIM, DIM, DROPOUT,layer)\n",
    "dec = Decoder(len(HINDI.vocab), EMB_DIM, DIM, DROPOUT,layer)\n",
    "\n",
    "model = Model(enc, dec, device).to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "Kl6OsialhY8n"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = HINDI.vocab.stoi[HINDI.pad_token])\n",
    "training_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "sEZ-QhTFhbcI",
    "outputId": "33193fad-c124-474f-c1dc-f8ab5d598739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 5.289 | Train PPL: 198.055\n",
      "\t Val. Loss: 8.486 |  Val. PPL: 4844.964\n",
      "Epoch: 02\n",
      "\tTrain Loss: 5.242 | Train PPL: 189.023\n",
      "\t Val. Loss: 8.294 |  Val. PPL: 4001.621\n",
      "Epoch: 03\n",
      "\tTrain Loss: 5.234 | Train PPL: 187.538\n",
      "\t Val. Loss: 8.329 |  Val. PPL: 4142.161\n",
      "Epoch: 04\n",
      "\tTrain Loss: 5.230 | Train PPL: 186.766\n",
      "\t Val. Loss: 8.341 |  Val. PPL: 4192.256\n",
      "Epoch: 05\n",
      "\tTrain Loss: 5.228 | Train PPL: 186.484\n",
      "\t Val. Loss: 8.493 |  Val. PPL: 4878.852\n",
      "Epoch: 06\n",
      "\tTrain Loss: 5.226 | Train PPL: 186.005\n",
      "\t Val. Loss: 8.558 |  Val. PPL: 5209.070\n",
      "Epoch: 07\n",
      "\tTrain Loss: 5.223 | Train PPL: 185.535\n",
      "\t Val. Loss: 8.726 |  Val. PPL: 6160.730\n",
      "Epoch: 08\n",
      "\tTrain Loss: 5.221 | Train PPL: 185.167\n",
      "\t Val. Loss: 8.562 |  Val. PPL: 5230.277\n",
      "Epoch: 09\n",
      "\tTrain Loss: 5.218 | Train PPL: 184.525\n",
      "\t Val. Loss: 8.446 |  Val. PPL: 4657.462\n",
      "Epoch: 10\n",
      "\tTrain Loss: 5.288 | Train PPL: 198.037\n",
      "\t Val. Loss: 8.633 |  Val. PPL: 5611.477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-174f07b546e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtraining_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-5caa66d07d11>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flattening the sentence_length x batch_size dimensions because cross entropy loss needs 1d shape and also removing the sos token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "best_valid_loss = 10000000000\n",
    "for epoch in range(0,N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_data_iterator, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, validation_data_iterator, criterion)\n",
    "    training_losses.append(np.exp(train_loss))\n",
    "    validation_losses.append(np.exp(valid_loss))\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'encoder_decoder_subword.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UF_luaMnhf9y",
    "outputId": "46a2ae56-bc0a-426d-9751-b6741dbe49bf"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('encoder_decoder_subword.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_data_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "gyqH-o8m8IU-",
    "outputId": "29a668d9-0c71-49ec-be70-4d762ec36b85"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_losses, label = 'Training')\n",
    "plt.plot(validation_losses, label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "P_vjw1GN8etv",
    "outputId": "288a6366-c018-451e-eee3-b594249d7164"
   },
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    tokens = english_tokenizer(text)\n",
    "    tokens = [ENGLISH.init_token] + tokens + [ENGLISH.eos_token]\n",
    "    src_indexes = [ENGLISH.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src_tensor = src_tensor.reshape(-1,1)\n",
    "\n",
    "    output = model(src_tensor, src_tensor)\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output.view(-1, output_dim)\n",
    "    indices = torch.argmax(output,dim=1).tolist()\n",
    "    return [HINDI.vocab.itos[x] for x in indices]\n",
    "\n",
    "print(translate(\"man speaking native language:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "olz8ioFy8hYn",
    "outputId": "0ce96f1a-e9c3-4a30-d445-d820f3060b9c"
   },
   "outputs": [],
   "source": [
    "for i, torch_sentence_pair in enumerate(test_data):\n",
    "  \n",
    "    eng_sentence = vars(torch_sentence_pair)[\"english\"]\n",
    "    hin_sentence = vars(torch_sentence_pair)[\"hindi\"]\n",
    "\n",
    "\n",
    "    hindi_predicted = translate(\" \".join(eng_sentence))\n",
    "    hindi_predicted = \" \".join(list(filter(lambda x: x != '<eos>' and x!= '<unk>', hindi_predicted))[1:])\n",
    "\n",
    "    print(eng_sentence)\n",
    "    print(hindi_predicted)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    if i == 500:\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FOiWTRgBmWH"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "translated_sentences = open('Paper1.csv','w')\n",
    "csvwriter = csv.writer(translated_sentences)\n",
    "csvwriter.writerow(['Input','Predicted','Actual'])  \n",
    "\n",
    "for i, (eng_sentence, hin_sentence) in enumerate(zip(test_data_sent[\"english\"], test_data_sent[\"hindi\"])):\n",
    "\n",
    "  hindi_predicted = translate(eng_sentence)\n",
    "  hindi_predicted = \" \".join(list(filter(lambda x: x != '<eos>', hindi_predicted))[1:]).strip()\n",
    "  csvwriter.writerow([eng_sentence.strip(), hindi_predicted.strip(), hin_sentence.strip()])\n",
    "\n",
    "  # print(eng_sentence.strip())\n",
    "  # print(hindi_predicted)\n",
    "  # print()\n",
    "  # print()\n",
    "\n",
    "translated_sentences.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gks5XHEdBsmm",
    "outputId": "7472fc72-6fb9-4784-b111-67321e6c6049"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# translated_sentences = open('Paper2.csv','w')\n",
    "# csvwriter = csv.writer(translated_sentences)\n",
    "# csvwriter.writerow(['Input','Predicted','Actual'])  \n",
    "\n",
    "import nltk\n",
    "\n",
    "bleu_i = []\n",
    "bleu_o = []\n",
    "\n",
    "for i, (eng_sentence, hin_sentence) in enumerate(zip(test_data_sent[\"english\"], test_data_sent[\"hindi\"])):\n",
    "\n",
    "  hindi_predicted = translate(eng_sentence)\n",
    "  hindi_predicted = \" \".join(list(filter(lambda x: x != '<eos>', hindi_predicted))[1:]).strip()\n",
    "  # csvwriter.writerow([eng_sentence.strip(), hindi_predicted.strip(), hin_sentence.strip()])\n",
    "\n",
    "  bleu_i.append(hin_sentence.strip())\n",
    "  bleu_o.append(hindi_predicted.strip())\n",
    "\n",
    "  # print(eng_sentence.strip())\n",
    "  # print(hindi_predicted)\n",
    "  # print()\n",
    "  # print()\n",
    "\n",
    "# translated_sentences.close()\n",
    "\n",
    "BLEU_scores = []\n",
    "\n",
    "for i in range(len(bleu_i)):\n",
    "  BLEU_scores.append(nltk.translate.bleu_score.sentence_bleu(bleu_i[i], bleu_o[i], smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method7))\n",
    "  \n",
    "print(\"Average BLEU Score:\", np.mean(BLEU_scores))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nlp_2.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
